{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "text_raw = json.load(open(\"/Users/andrewloh/Documents/Columbia/Columbia_Class/Spring 2019/G5099 - Thesis/Code/TV_specificbands.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['60Min', 'AllInHayes', 'Cooper360', 'ErinBurnett', 'FaceTheNation', 'Greta-Hume', 'Hannity', 'HardballMatthews', 'Maddow', 'NBCNightly', 'OReilly'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_raw.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = []\n",
    "for i in text_raw.keys():\n",
    "    for j in text_raw[i]:\n",
    "        doc_list.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.extend((\"hardball\", \"outfront\", \"unidentified\", \"clip\", \"crosstalk\", \"video\", \"percent\"))\n",
    "\n",
    "def clean_tokens(doc_list):\n",
    "    tokenized_list = []\n",
    "    for doc in doc_list:\n",
    "        cleaned_tokens = []\n",
    "        for token in doc:\n",
    "            if token.isalpha() == True:\n",
    "                if token.lower() not in stopwords:\n",
    "                    if \"http\" not in token:\n",
    "                        cleaned_tokens.append(token)\n",
    "\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in cleaned_tokens]\n",
    "        tokenized_list.append(stemmed_tokens)\n",
    "    \n",
    "    return tokenized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_tokens = clean_tokens(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "common_dictionary = Dictionary(stemmed_tokens)\n",
    "common_dictionary.filter_extremes(no_below=15, no_above=0.3, keep_n=100000)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in stemmed_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "lda = LdaModel(common_corpus, num_topics=6, id2word = common_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"women\" + 0.005*\"job\" + 0.005*\"tax\" + 0.004*\"help\" + 0.004*\"illeg\" + 0.004*\"messag\" + 0.004*\"white\" + 0.004*\"cnn\" + 0.004*\"media\" + 0.004*\"whether\"'),\n",
       " (1,\n",
       "  '0.008*\"white\" + 0.006*\"million\" + 0.006*\"democrat\" + 0.006*\"sort\" + 0.005*\"polici\" + 0.005*\"media\" + 0.004*\"latino\" + 0.004*\"attack\" + 0.004*\"cnn\" + 0.004*\"big\"'),\n",
       " (2,\n",
       "  '0.005*\"refuge\" + 0.005*\"women\" + 0.005*\"help\" + 0.005*\"guy\" + 0.004*\"plan\" + 0.004*\"night\" + 0.004*\"big\" + 0.004*\"border\" + 0.004*\"hear\" + 0.004*\"media\"'),\n",
       " (3,\n",
       "  '0.006*\"parti\" + 0.006*\"chang\" + 0.005*\"week\" + 0.005*\"keep\" + 0.005*\"illeg\" + 0.005*\"guy\" + 0.004*\"big\" + 0.004*\"night\" + 0.004*\"polici\" + 0.004*\"million\"'),\n",
       " (4,\n",
       "  '0.007*\"women\" + 0.006*\"big\" + 0.005*\"polici\" + 0.005*\"speech\" + 0.005*\"chang\" + 0.004*\"parti\" + 0.004*\"night\" + 0.004*\"whole\" + 0.004*\"sort\" + 0.004*\"attack\"'),\n",
       " (5,\n",
       "  '0.007*\"border\" + 0.007*\"illeg\" + 0.005*\"report\" + 0.005*\"guy\" + 0.005*\"white\" + 0.004*\"women\" + 0.004*\"job\" + 0.004*\"speech\" + 0.004*\"watch\" + 0.004*\"part\"')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"unidentifi\" + 0.007*\"crosstalk\" + 0.005*\"york\" + 0.005*\"democrat\" + 0.005*\"guy\" + 0.005*\"women\" + 0.004*\"war\" + 0.004*\"fight\" + 0.004*\"former\" + 0.004*\"keep\"'),\n",
       " (1,\n",
       "  '0.006*\"crosstalk\" + 0.006*\"parti\" + 0.006*\"commun\" + 0.005*\"ever\" + 0.005*\"latino\" + 0.005*\"women\" + 0.005*\"applaus\" + 0.005*\"night\" + 0.005*\"bet\" + 0.005*\"job\"'),\n",
       " (2,\n",
       "  '0.009*\"percent\" + 0.009*\"latino\" + 0.007*\"big\" + 0.007*\"parti\" + 0.007*\"sort\" + 0.005*\"seen\" + 0.005*\"crosstalk\" + 0.005*\"guy\" + 0.005*\"white\" + 0.005*\"democrat\"'),\n",
       " (3,\n",
       "  '0.009*\"women\" + 0.006*\"crosstalk\" + 0.005*\"job\" + 0.005*\"million\" + 0.005*\"report\" + 0.005*\"may\" + 0.004*\"attack\" + 0.004*\"told\" + 0.004*\"big\" + 0.004*\"cnn\"'),\n",
       " (4,\n",
       "  '0.005*\"speech\" + 0.004*\"fox\" + 0.004*\"women\" + 0.004*\"former\" + 0.004*\"three\" + 0.004*\"open\" + 0.004*\"quot\" + 0.004*\"told\" + 0.004*\"part\" + 0.004*\"feel\"'),\n",
       " (5,\n",
       "  '0.007*\"speech\" + 0.006*\"percent\" + 0.006*\"sort\" + 0.005*\"night\" + 0.005*\"week\" + 0.005*\"matter\" + 0.005*\"media\" + 0.005*\"polici\" + 0.004*\"report\" + 0.004*\"tax\"'),\n",
       " (6,\n",
       "  '0.009*\"illeg\" + 0.009*\"million\" + 0.008*\"crosstalk\" + 0.007*\"women\" + 0.007*\"job\" + 0.006*\"polici\" + 0.005*\"care\" + 0.005*\"help\" + 0.005*\"media\" + 0.005*\"sanctuari\"'),\n",
       " (7,\n",
       "  '0.006*\"polici\" + 0.006*\"women\" + 0.005*\"sort\" + 0.005*\"thought\" + 0.005*\"chang\" + 0.005*\"illeg\" + 0.005*\"stay\" + 0.004*\"care\" + 0.004*\"hear\" + 0.004*\"watch\"'),\n",
       " (8,\n",
       "  '0.010*\"crosstalk\" + 0.006*\"refuge\" + 0.006*\"black\" + 0.006*\"unidentifi\" + 0.005*\"guy\" + 0.005*\"ever\" + 0.005*\"democrat\" + 0.004*\"thought\" + 0.004*\"percent\" + 0.004*\"polici\"'),\n",
       " (9,\n",
       "  '0.007*\"unidentifi\" + 0.007*\"parti\" + 0.006*\"cnn\" + 0.006*\"polici\" + 0.005*\"refuge\" + 0.005*\"crosstalk\" + 0.005*\"white\" + 0.005*\"million\" + 0.005*\"guy\" + 0.004*\"help\"'),\n",
       " (10,\n",
       "  '0.012*\"crosstalk\" + 0.006*\"senat\" + 0.006*\"white\" + 0.006*\"done\" + 0.006*\"attack\" + 0.006*\"democrat\" + 0.006*\"women\" + 0.006*\"wall\" + 0.005*\"chang\" + 0.005*\"million\"'),\n",
       " (11,\n",
       "  '0.042*\"unidentifi\" + 0.007*\"illeg\" + 0.005*\"democrat\" + 0.005*\"million\" + 0.005*\"border\" + 0.004*\"crosstalk\" + 0.004*\"long\" + 0.004*\"wall\" + 0.004*\"love\" + 0.004*\"build\"')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"ail\" + 0.004*\"comey\" + 0.004*\"wind\" + 0.004*\"mexican\" + 0.004*\"storm\" + 0.004*\"roger\" + 0.003*\"hurrican\" + 0.003*\"enforc\" + 0.003*\"suspect\" + 0.003*\"Al\"'),\n",
       " (1,\n",
       "  '0.007*\"obamacar\" + 0.005*\"comey\" + 0.005*\"storm\" + 0.004*\"hurrican\" + 0.004*\"king\" + 0.003*\"michael\" + 0.003*\"mayor\" + 0.003*\"joke\" + 0.003*\"email\" + 0.003*\"nasti\"'),\n",
       " (2,\n",
       "  '0.006*\"radic\" + 0.005*\"islam\" + 0.005*\"wikileak\" + 0.004*\"michel\" + 0.004*\"comey\" + 0.003*\"veteran\" + 0.003*\"melania\" + 0.003*\"defeat\" + 0.003*\"matt\" + 0.003*\"hurrican\"'),\n",
       " (3,\n",
       "  '0.006*\"russia\" + 0.004*\"korea\" + 0.004*\"bet\" + 0.004*\"comey\" + 0.004*\"michel\" + 0.004*\"melania\" + 0.003*\"kati\" + 0.003*\"classifi\" + 0.003*\"birther\" + 0.003*\"fraud\"'),\n",
       " (4,\n",
       "  '0.006*\"kain\" + 0.004*\"wikileak\" + 0.003*\"steve\" + 0.003*\"birther\" + 0.003*\"ail\" + 0.003*\"elizabeth\" + 0.003*\"warren\" + 0.003*\"michael\" + 0.003*\"pastor\" + 0.003*\"roger\"'),\n",
       " (5,\n",
       "  '0.007*\"comey\" + 0.005*\"melania\" + 0.004*\"michel\" + 0.004*\"matt\" + 0.003*\"storm\" + 0.003*\"classifi\" + 0.003*\"machado\" + 0.003*\"russia\" + 0.003*\"veteran\" + 0.003*\"fascin\"'),\n",
       " (6,\n",
       "  '0.004*\"kain\" + 0.004*\"mayor\" + 0.004*\"comey\" + 0.004*\"christi\" + 0.003*\"congressman\" + 0.003*\"medic\" + 0.003*\"wikileak\" + 0.003*\"email\" + 0.003*\"racist\" + 0.003*\"chri\"'),\n",
       " (7,\n",
       "  '0.006*\"comey\" + 0.004*\"wikileak\" + 0.004*\"michel\" + 0.004*\"syrian\" + 0.003*\"obamacar\" + 0.003*\"michael\" + 0.003*\"islam\" + 0.003*\"enforc\" + 0.003*\"mile\" + 0.003*\"storm\"'),\n",
       " (8,\n",
       "  '0.005*\"comey\" + 0.005*\"michel\" + 0.004*\"wikileak\" + 0.004*\"mayor\" + 0.004*\"classifi\" + 0.003*\"brief\" + 0.003*\"suspect\" + 0.003*\"jersey\" + 0.003*\"assault\" + 0.003*\"joke\"'),\n",
       " (9,\n",
       "  '0.006*\"comey\" + 0.005*\"wikileak\" + 0.004*\"nasti\" + 0.004*\"storm\" + 0.004*\"michel\" + 0.003*\"pastor\" + 0.003*\"radic\" + 0.003*\"leak\" + 0.003*\"classifi\" + 0.003*\"dinner\"'),\n",
       " (10,\n",
       "  '0.006*\"veteran\" + 0.004*\"deplor\" + 0.004*\"kain\" + 0.004*\"radic\" + 0.003*\"wikileak\" + 0.003*\"islam\" + 0.003*\"birther\" + 0.003*\"child\" + 0.003*\"suspect\" + 0.003*\"gari\"'),\n",
       " (11,\n",
       "  '0.004*\"mayor\" + 0.004*\"storm\" + 0.003*\"michel\" + 0.003*\"wind\" + 0.003*\"jersey\" + 0.003*\"matt\" + 0.003*\"comey\" + 0.003*\"steve\" + 0.003*\"kain\" + 0.003*\"classifi\"')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topicdist(model, text_raw_source):\n",
    "    source_list = []\n",
    "    for i in text_raw_source:\n",
    "        source_list.append(i)\n",
    "    source_tokenized = clean_tokens(source_list)\n",
    "    \n",
    "    source_topics = {}\n",
    "    \n",
    "    for individual, resume in enumerate(source_tokenized):\n",
    "        NEW_BOW = common_dictionary.doc2bow(document = resume)\n",
    "        topic_dist = model.get_document_topics(NEW_BOW)\n",
    "        indiv_dict = {}\n",
    "        for i in topic_dist:\n",
    "            indiv_dict[str(i[0])] = i[1]\n",
    "    \n",
    "    source_topics[\"Document_\" + str(individual)] = indiv_dict\n",
    "    \n",
    "    import pandas as pd\n",
    "    topic_df = pd.DataFrame(source_topics)\n",
    "    topic_sum = topic_df.sum(axis = 1)\n",
    "    \n",
    "    return ((topic_sum/len(topic_df.columns.values)).sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    0.013009\n",
       "1    0.976281\n",
       "dtype: float64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_topicdist(lda, text_raw[\"Hannity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    0.025718\n",
       "1    0.116699\n",
       "3    0.852860\n",
       "dtype: float64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_topicdist(lda, text_raw[\"HardballMatthews\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
